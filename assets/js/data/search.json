[ { "title": "Creating a Simple Application (Backend + Frontend) on a Local Minikube K8S Cluster", "url": "/posts/simple_backend_frontend_local_minikube_k8s_cluster/", "categories": "IT, Kubernetes", "tags": "it, kubernetes, k8s, minikube, micronaut, kotlin, reactts, nginx", "date": "2023-07-25 07:00:00 +0000", "snippet": "In this article, you will learn how to create a simple web application using Micronaut with Kotlin for the backend andReact with TypeScript for the frontend. We will then configure a local Kubernet...", "content": "In this article, you will learn how to create a simple web application using Micronaut with Kotlin for the backend andReact with TypeScript for the frontend. We will then configure a local Kubernetes (K8S) environment using Minikube todeploy our application. Additionally, you will learn how to prepare the necessary configuration files for K8S and deployan NGINX proxy to efficiently manage traffic to our application. Finally, we will add access to the application throughlocalhost.Setting up the EnvironmentBefore we begin working on the application, make sure you have the following tools installed: Java Development Kit (JDK) - for Micronaut and Kotlin Node.js and npm - for React Docker - for building container images kubectl - for managing Kubernetes clusters Minikube - for the local Kubernetes clusterEnsure that Minikube is correctly installed and running on your local machine.minikube startSetting up Kubernetes Dashboard in MinikubeKubernetes Dashboard is a web-based user interface that allows you to manage and monitor your Kubernetes cluster. To usethe Dashboard, we need to set it up in Minikube and access it through a web browser.Enable the Dashboard addon in Minikube:minikube addons enable dashboardAccess the Dashboard using kubectl:minikube dashboardThis command will open the Kubernetes Dashboard in your default web browser.Generating Micronaut + Kotlin ApplicationMicronaut is a lightweight framework for building microservices and applications in languages like Kotlin and Java.Let‚Äôs start by creating the backend part of our application.Install Micronaut CLI if you haven‚Äôt already:npm install -g micronautGenerate a new Micronaut project with Kotlin:mn create-app backend --lang=kotlinGenerating ReactTS ApplicationNext, we will create the frontend of our application using React with TypeScript.Generate a new React project with TypeScript:npx create-react-app frontend --template typescriptPreparing K8S clusterWith the backend and frontend applications prepared, let‚Äôs proceed to deploy them on the Minikube Kubernetes cluster.Build container images for the backend and frontend applicationsIn the backend directory add DockerfileFROM eclipse-temurin:17ADD build/libs/backend-0.1-all.jar backend.jarENTRYPOINT [\"java\", \"-jar\", \"/backend.jar\"]and execute:./gradlew shadowJardocker build -t my-backend-image .minikube image load my-backend-imageIn the frontend directory add DockerfileFROM node:18WORKDIR /appCOPY package*.json ./RUN npm installCOPY . ./CMD [\"npm\", \"start\"]and execute:docker build -t my-frontend-image .minikube image load my-frontend-imagePrepare configuration files for K8SSample configuration for the backend (backend-deployment.yaml):apiVersion: apps/v1kind: Deploymentmetadata: name: \"backend\"spec: selector: matchLabels: app: \"backend\" template: metadata: labels: app: \"backend\" spec: containers: - name: \"backend\" image: \"my-backend-image:latest\" imagePullPolicy: Never ports: - name: http containerPort: 8080 readinessProbe: httpGet: path: /health/readiness port: 8080 initialDelaySeconds: 5 timeoutSeconds: 3 livenessProbe: httpGet: path: /health/liveness port: 8080 initialDelaySeconds: 5 timeoutSeconds: 3 failureThreshold: 10---apiVersion: v1kind: Servicemetadata: name: \"backend\"spec: selector: app: \"backend\" type: ClusterIP ports: - protocol: \"TCP\" port: 8080 targetPort: 8080‚èéSample configuration for the frontend (frontend-deployment.yaml):apiVersion: apps/v1kind: Deploymentmetadata: name: \"frontend\"spec: selector: matchLabels: app: \"frontend\" template: metadata: labels: app: \"frontend\" spec: containers: - name: \"frontend\" image: \"my-frontend-image:latest\" imagePullPolicy: Never ports: - name: http containerPort: 3000---apiVersion: v1kind: Servicemetadata: name: \"frontend\"spec: selector: app: \"frontend\" type: ClusterIP ports: - protocol: TCP port: 3000 targetPort: 3000‚èéDeploying to Minikube K8S:Ensure that Minikube is running and kubectl is correctly configured to work with the Minikube cluster.Execute:kubectl apply -f backend-deployment.yamlkubectl apply -f frontend-deployment.yamlPreparing Files for NGINX Proxy DeploymentTo effectively manage traffic to our backend and frontend applications, we will use an NGINX proxy.Prepare the configuration file for NGINX (nginx.conf):events { worker_connections 1024;}http { upstream backend { server backend:8080; } upstream frontend { server frontend:3000; } server { listen 80; location /api/ { proxy_pass http://backend; } location / { proxy_pass http://frontend; } }}Dockerfile which uses above configuration:FROM nginx:alpineRUN rm -rf /usr/share/nginx/html/* &amp;&amp; rm -rf /etc/nginx/conf.d/default.conf &amp;&amp; rm -rf /etc/nginx/nginx.confCOPY nginx.conf /etc/nginxBuild the image and add to Minikube:docker build -t nginx .minikube image load nginxAnd configuration file for K8S deployment (nginx-deployment.yaml):apiVersion: apps/v1kind: Deploymentmetadata: name: \"nginx\"spec: selector: matchLabels: app: \"nginx\" template: metadata: labels: app: \"nginx\" spec: containers: - name: \"nginx\" image: \"nginx:latest\" imagePullPolicy: Never ports: - name: http containerPort: 80---apiVersion: v1kind: Servicemetadata: name: \"nginx\"spec: selector: app: \"nginx\" type: NodePort ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 31000Execute:kubectl apply -f nginx-config.yamlNow you should be able to see all deployments and services in kubernetes dashboard.Accessing the Application through LocalhostTo access the application through localhost, we need to expose only the NGINX proxy service and configure it to routetraffic internally within the cluster.Expose the NGINX proxy service:minikube service nginx --urlNow you should be able to access the application through your browser using the URL given by command above.Please note that the backend and frontend services are not directly accessible from localhost. The NGINX proxy willhandle routing traffic internally within the cluster, providing a seamless experience for the users.SummaryCongratulations! You have successfully created a simple application consisting of a backend built with Micronaut andKotlin and a frontend using React with TypeScript. Additionally, you have learned how to prepare configuration files forthe Minikube Kubernetes cluster, deploy an NGINX proxy to efficiently manage traffic to the applications, and access theapplication through localhost. You can now experiment with expanding your application or further explore Kubernetes andweb application development. Happy learning and developing!" }, { "title": "Bootstrap project with Spring Boot, Kotest, Testcontainers & MongoDB", "url": "/posts/example-spring-kotest-testcontainers-mongodb-project/", "categories": "IT, Spring Framework", "tags": "it, spring framework, kotest, testcontainers, mongodb", "date": "2023-04-06 05:00:00 +0000", "snippet": "I use Spring Initializr to generate project structure and dependencies when I create a new project.After that, I need to add some additional dependencies and configuration to make it work with Kote...", "content": "I use Spring Initializr to generate project structure and dependencies when I create a new project.After that, I need to add some additional dependencies and configuration to make it work with Kotest and Testcontainers.I must also add some configuration to make it work with MongoDB.I created a project template with all dependencies and configuration needed to start working with Spring Boot, Kotest, Testcontainers &amp; MongoDB.Project templateThe project template is available on my GitHub.What is essential in the project: Dependencies in build.gradle.kts: implementation(\"org.springframework.boot:spring-boot-starter-data-mongodb\") implementation(\"org.springframework.boot:spring-boot-starter-web\") implementation(\"com.fasterxml.jackson.module:jackson-module-kotlin\") implementation(\"org.jetbrains.kotlin:kotlin-reflect\") testImplementation(\"org.springframework.boot:spring-boot-starter-test\") testImplementation(\"org.testcontainers:junit-jupiter\") testImplementation(\"org.testcontainers:mongodb\") testImplementation(\"io.kotest:kotest-runner-junit5-jvm:5.5.5\") testImplementation(\"io.kotest:kotest-assertions-core-jvm:5.5.5\") testImplementation(\"io.kotest.extensions:kotest-extensions-spring:1.1.2\") testImplementation(\"io.kotest.extensions:kotest-extensions-testcontainers:1.3.4\") IntegrationSpec which is the base for all integration tests:@SpringBootTest@ActiveProfiles(\"integration\")@AutoConfigureMockMvc@Testcontainersabstract class IntegrationSpec(body: ShouldSpec.() -&gt; Unit = {}) : ShouldSpec(body) { override fun extensions(): List&lt;Extension&gt; = listOf(SpringExtension) @Autowired lateinit var mockMvc: MockMvc @Autowired lateinit var mongoOperations: MongoOperations override suspend fun beforeEach(testCase: TestCase) { super.beforeEach(testCase) mongoOperations.collectionNames.forEach { mongoOperations.dropCollection(it) } } companion object { @Container @JvmField var container = MongoDBContainer(DockerImageName.parse(\"mongo:6\")) init { container.start() } @DynamicPropertySource @JvmStatic fun mongoDbProperties(registry: DynamicPropertyRegistry) { registry.add(\"spring.data.mongodb.uri\") { container.replicaSetUrl } } }}With such a configuration, you can start writing tests using MongoDB runnig in a Docker container and create the whole Spring context." }, { "title": "Quick dump MongoDB to BigQuery using Dataflow", "url": "/posts/quick-dump-mongodb-to-bigquery-using-dataflow/", "categories": "IT, MongoDB", "tags": "it, mongodb, gcp, bigquery, dataflow", "date": "2023-03-16 07:00:00 +0000", "snippet": "In this post, I want to share one way of dumping MongoDB collection into the BigQuery table to analyze data using Google Cloud Platform resources.What is Dataflow?Dataflow is one of the services on...", "content": "In this post, I want to share one way of dumping MongoDB collection into the BigQuery table to analyze data using Google Cloud Platform resources.What is Dataflow?Dataflow is one of the services on the Google Cloud Platform, which allows processing stream and batch data that‚Äôs serverless, fast, andcost-effective.More about the Dataflow you can see here: Dataflow.Google provides plenty of ready-to-use templates (list of available templates you can seehere: Provided templates.Few of them use MongoDB: [Streaming] MongoDB to BigQuery (CDC) [Streaming] Pub/Sub to MongoDB [Batch] BigQuery to MongoDB [Batch] MongoDB to BigQueryI want to focus on the last one.MongoDB to BigQuery templateThanks to this template, you can easily dump MongoDB collection into a BigQuery table by running a job.How to do this is very well described on the GCPwebsite: MongoDB Atlas and BigQuery dataflow templates.So I won‚Äôt copy it here.I show you a command (uses gcloud) that allows you to run such processing.gcloud dataflow flex-template run my-dataflow-name \\ --template-file-gcs-location gs://dataflow-templates-europe-west1/latest/flex/MongoDB_to_BigQuery \\ --region europe-west1 \\ --parameters mongoDbUri=mongodb+srv://username:password@host:port/database_name?authSource=admin,database=database_name,collection=collection_name,outputTableSpec=project:dataset.table,userOption=NONE,javascriptDocumentTransformGcsPath=gs://pathgcloud dataflow flex-template run my-dataflow-name runs new Dataflow processing with given name.Parameters template-file-gcs-location and region are global and allow you to specify which template you want to use and on which region it shouldrun.The parameter parameters is specific to the selected template. In it, you canspecify: mongoDbUri, database, collection, outputTableSpec, userOption, javascriptDocumentTransformGcsPath.The first four, in my opinion, are quite clear. I‚Äôll tell you about the last two.Result table schemauserOption parameter is responsible for the result schema of the dump.You have two options here: NONE - stores the whole document as JSON string. FLATTEN - flatten the document to the first level. Nested documents are stored as JSON string.With the default NONE option, the result schema looks like this:With example data: id source_data timestamp e6570932-b152-4898-95b9-a1debdab685e {‚Äú_id‚Äù:‚Äùe6570932-b152-4898-95b9-a1debdab685e‚Äù, ‚Äúname‚Äù:‚ÄùJakub‚Äù} 2023-03-15 16:59:25.982000 UTC At first glance, such a dump may seem challenging to use, but I will show you how to use it in the following.UDF - map data on the flyDataflow templates (globally, not only in MongoDB templates) can extend it using User Defined Functions.UDFs are functions written in JavaScript like this:function process(inJson) { var obj = JSON.parse(inJson); // Map object here return JSON.stringify(obj);}As a parameter, it takes JSON string, and as a result, it expects JSON string.More about using UDFs you can readhere: Extend your Dataflow template with UDFs.It may be helpful when you want to: extract from an array, calculate, drop some data, etc.Working with JSON data in BigQueryBigQuery supports JSON column type.Using function PARSE_JSON we can change from JSON string toJSON column type.After that, you can work with JSON columns like describedhere: Working with JSON data in GoogleSQL.Example query:WITH dump_with_parser_json AS (SELECT id, PARSE_JSON(source_data) AS source_data, timestampFROM `sc-6014-pmd-dev.temp.tecdoc_relations`)SELECT source_data._idFROM dump_with_parser_json LIMIT 1000The whole dump process and the possibility of working with JSON data in BigQuery may be helpful when you want to dump your database quickly intoBigQuery and analyze it there." }, { "title": "Deploy Spark-Kotlin project on GCP Dataproc cluster", "url": "/posts/spark-kotlin-on-gcp/", "categories": "IT, Apache Spark", "tags": "it, apache spark, kotlin, gcp", "date": "2023-02-16 07:00:00 +0000", "snippet": "IntroductionIn the previous post ‚ÄúHow to create an example Spark-Kotlin-Gradle project?‚Äù Ishowed you how to create an example Apache Spark project written in Kotlin lang andusing Gradle build syste...", "content": "IntroductionIn the previous post ‚ÄúHow to create an example Spark-Kotlin-Gradle project?‚Äù Ishowed you how to create an example Apache Spark project written in Kotlin lang andusing Gradle build system.You can run this project locally, but Spark was designed to work with a lotof data and a big, distributed compute cluster so to unleash the full potential of this tool you may want to run it on a cloud. For example on GoogleCloud Platform.In this post, I‚Äôll show you how to run an example Spark-Kotlin-Gradle project on GCPusing gcloud command line tool.Repository with an example project is availableon my GitHub.Adapt project to GCPVersionsGCP delivers specific VM images which contain particular versions of Java, Scala, Spark, etc.The newest image is 2.1.2-debian11 with the following: Apache Spark 3.3.0 Java 11 Scala 2.12.14You can see a list of available components version here.Because of that, you have to set specific versions of Java and dependencies in build.gradle.kts: JVM version 11 (jvmToolchain(11)) Apache Spark version 3.3.0 with Scala 2.12 (compileOnly(\"org.apache.spark:spark-sql_2.12:3.3.0\")) Kotlin Spark API (implementation(\"org.jetbrains.kotlinx.spark:kotlin-spark-api_3.3.0_2.12:1.2.3\"))Spark BigQuery ConnectorFor using GCP tools like BigQuery, Google Cloud Storage, etc., from Apache Spark you should use additionallibrary Spark BigQuery Connector.It has very well described README.md, so I encourage you to familiarize yourself with it.You have to add a dependency with a matching version:implementation(\"com.google.cloud.spark:spark-bigquery_2.12:0.28.0\")Example code &amp; dataIn the example project, I want to show you how to read &amp; write data using Google Cloud Storage bucket.I downloaded publicly available dataof ESA air pollution measurements in Poland and put ininto CSV, which you can copy intobucket and load in Spark.Let‚Äôs look at the example code, initially only for data containers.CSV is a flat file so that you can load data into some DTO classes:data class MeasurementDto( val school_name: String, val school_street: String?, val school_post_code: String, val school_city: String, val school_longitude: String, val school_latitude: String, val data_humidity_avg: String, val data_pressure_avg: String, val data_temperature_avg: String, val data_pm10_avg: String, val data_pm25_avg: String, val timestamp: String) : Serializablewhich you can map into more domain models like:data class Measurement( val school: School, val data: MeasurementData, val timestamp: Timestamp) : Serializabledata class School( val name: String, val street: String?, val postCode: String, val city: String, val longitude: Double, val latitude: Double) : Serializabledata class MeasurementData( val humidityAverage: Double, val pressureAverage: Double, val temperatureAverage: Double, val pm10Average: Double, val pm25Average: Double) : Serializableusing function:data class MeasurementDto( ...) : Serializable { fun toDomainObject(): Measurement = Measurement( school = School( name = school_name, street = school_street, postCode = school_post_code, city = school_city, longitude = school_longitude.toDouble(), latitude = school_latitude.toDouble() ), data = MeasurementData( humidityAverage = data_humidity_avg.toDouble(), pressureAverage = data_pressure_avg.toDouble(), temperatureAverage = data_temperature_avg.toDouble(), pm10Average = data_pm10_avg.toDouble(), pm25Average = data_pm25_avg.toDouble() ), timestamp = Timestamp.valueOf(timestamp) )}Now, you can look at the code executed by Spark:fun main() { withSpark(appName = \"Measurements per city counter\") { val inputPath = \"gs://example-data/esa_air_pollution_measurements_poland.csv\" val resultPath = \"gs://example-data/measurements_per_city.csv\" println(\"Starting job...\") spark .read() .setDelimiter(\";\") .firstRowAsHeader() .csv(inputPath) .`as`&lt;MeasurementDto&gt;() .map { it.toDomainObject() } .groupByKey { it.school.city } .count() .repartition(1) .write() .csv(resultPath) println(\"Job done.\") }}private fun DataFrameReader.setDelimiter(delimiter: String) = this.option(\"delimiter\", delimiter)private fun DataFrameReader.firstRowAsHeader() = this.option(\"header\", \"true\")The job is simple: Reads data from Google Cloud Storage bucket (DTO class) Maps to the domain model Groups by city Counts elements in group Saves result in GCSDeployment steps First of all, you must have a package ready to be deployed:./gradlew clean &amp;&amp; ./gradlew shadowJar Secondly, you must have a place to share this package on GCP and upload data. For that, you have to create your own Google Cloud Storagebucket (something like a folder in the cloud) and upload package &amp; data there:gsutil mb gs://example-datagsutil cp build/libs/spark-kotlin-on-gcp-1.0-SNAPSHOT-all.jar gs://example-data/gsutil cp data/esa_air_pollution_measurements_poland.csv gs://example-data/ Thirdly, Apache Spark requires compute instances on which it can run. GCP supplies creating such clusters, and it‚Äôs called Dataproc:gcloud dataproc clusters create my-cluster --region europe-west1 --image-version 2.1.2-debian11 Now, you are ready to submit Apache Spark job to run on the created cluster:gcloud dataproc jobs submit spark --cluster my-cluster --region europe-west1 --jars gs://example-data/spark-kotlin-on-gcp-1.0-SNAPSHOT-all.jar --class pl.jakubpradzynski.measurements.MeasurementsPerCityCounterJobKt When the job is done, you can clean up on GCP:gcloud -q dataproc clusters delete my-cluster --region europe-west1gsutil rm -r gs://example-dataAll those steps are available in the single shell script, which you cancheck here.SummaryIf you want to read more about it, I refer you to some documentation: Creating Dataproc cluster Write and run Spark Scala jobs on Dataproc Dataproc versioning Spark BigQuery ConnectorYou can use my example project fromGitHub: Example Spark-Kotlin-Gradle project.Above, I only describe a simple happy path of deployment. It‚Äôs worth exploring more about: different or more complex dataproc clusters, costoptimizations, more complex Spark jobs, or Spark optimization. In the future, I plan to cover some of those topics." }, { "title": "How to create an example Spark-Kotlin-Gradle project?", "url": "/posts/example-spark-kotlin-gradle-project/", "categories": "IT, Apache Spark", "tags": "it, apache spark, kotlin, gradle", "date": "2023-02-13 08:00:00 +0000", "snippet": "IntroductionDo you want to start using Apache Spark from the basics?Or you already use it and want to write jobs in Kotlin with Gradle.In this post, I will show you how tocreate example Spark-Kotli...", "content": "IntroductionDo you want to start using Apache Spark from the basics?Or you already use it and want to write jobs in Kotlin with Gradle.In this post, I will show you how tocreate example Spark-Kotlin-Gradle project from which you can start furtherwork with Apache Spark.If you need a ready-to-use project, there is one available on myGitHub here.Example Spark-Kotlin-Gradle projectGenerating projectInitially, I generated the project using IntelliJ IDEA (File &gt; New &gt; New project).Selected: Language: Kotlin Build system: Gradle JVM: 17 Gradle DSL: KotlinDependenciesIn such generated project, you need to define dependencies for Apache Spark and Kotlin API.dependencies { compileOnly(\"org.apache.spark:spark-sql_2.13:3.3.1\") // Apache Spark 3.3.1 for Scala 2.13 implementation(\"org.jetbrains.kotlinx.spark:kotlin-spark-api_3.3.1_2.13:1.2.3\") // Spark Kotlin API 1.2.3 for Apache Spark 3.3.1 (and Scala 2.13) implementation(\"com.fasterxml.jackson.core:jackson-core:2.14.2\") // Required for proper work by other dependencies testImplementation(\"io.kotest:kotest-runner-junit5:5.5.5\") // Kotest library for tests testImplementation(\"io.kotest:kotest-assertions-core:5.5.5\") // Kotest assertions for tests}You also have to add a plugin:plugins { id(\"com.github.johnrengelman.shadow\") version \"7.1.2\"}Which allows you to build fat jar with all required dependencies.May be required to enable the option:tasks.shadowJar { isZip64 = true}Because of the size of the jar.With all those dependencies, you will be ready to execute your job.Example jobCodeTo see if it works, you can create a simple job like this:import org.jetbrains.kotlinx.spark.api.dsOfimport org.jetbrains.kotlinx.spark.api.reduceKimport org.jetbrains.kotlinx.spark.api.withSparkimport java.io.Serializableimport java.math.BigDecimalimport java.math.RoundingMode.HALF_EVENimport kotlin.random.Randomfun main(args: Array&lt;String&gt;) { withSpark(appName = \"Example Spark-Kotlin-Gradle project\") { println(\"Staring example Kotlin-Spark project\") println(\"Program arguments: ${args.joinToString()}\") val randomPrices = (0..args.first().toInt()) .map { Price(randomBigDecimal()) } .toTypedArray() spark .dsOf(*randomPrices) .reduceK { price, price2 -&gt; Price(price.priceInDollars + price2.priceInDollars) } .let { println(\"Sum of random prices: ${it.priceInDollars.setScale(2, HALF_EVEN)}$\") } }}private fun randomBigDecimal() = Random.nextDouble(0.0, 99.99).toBigDecimal()data class Price(val priceInDollars: BigDecimal) : Serializable { operator fun plus(secondSummand: Price): Price { return Price(this.priceInDollars + secondSummand.priceInDollars) }}As you can see, it‚Äôs a standard Kotlin main function with arguments.It runs Apache Spark using the withSpark function. It accepts everything that you may need to run Apache Spark.Inside, you can use App arguments and refer to the spark object.This job: takes the first parameter as a number of randomly generated prices creates Dataset from those prices sums all prices using Spark into one variable prints the resultTestI overwrite the + operator for summing two Price objects to show you that it is possible to write tests for it.To check if it‚Äôs the correct implementation I created a test for it in Kotest:import io.kotest.core.spec.style.ShouldSpecimport io.kotest.matchers.shouldBeimport java.math.BigDecimalclass PriceTest : ShouldSpec({ should(\"sum two prices\") { // GIVEN val firstPriceInBigDecimal = BigDecimal.ONE val secondPriceInBigDecimal = BigDecimal.TEN val firstPrice = Price(firstPriceInBigDecimal) val secondPrice = Price(secondPriceInBigDecimal) // WHEN val priceSum = firstPrice + secondPrice // THEN priceSum.priceInDollars shouldBe (firstPriceInBigDecimal + secondPriceInBigDecimal) }})It‚Äôs also possible to write a test for code running on Spark, but it‚Äôs beyond the scope of this post.Execute the jobCommand lineTo run the job from the command line, you have to build the previously mentioned shadow jar:./gradlew shadowJarAfter that, you can run it with the \"10\" argument using the command:java -XX:+IgnoreUnrecognizedVMOptions \\ --add-opens=java.base/java.lang=ALL-UNNAMED \\ --add-opens=java.base/java.lang.invoke=ALL-UNNAMED \\ --add-opens=java.base/java.lang.reflect=ALL-UNNAMED \\ --add-opens=java.base/java.io=ALL-UNNAMED \\ --add-opens=java.base/java.net=ALL-UNNAMED \\ --add-opens=java.base/java.nio=ALL-UNNAMED \\ --add-opens=java.base/java.util=ALL-UNNAMED \\ --add-opens=java.base/java.util.concurrent=ALL-UNNAMED \\ --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED \\ --add-opens=java.base/sun.nio.ch=ALL-UNNAMED \\ --add-opens=java.base/sun.nio.cs=ALL-UNNAMED \\ --add-opens=java.base/sun.security.action=ALL-UNNAMED \\ --add-opens=java.base/sun.util.calendar=ALL-UNNAMED \\ --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED \\ -jar build/libs/example-spark-kotlin-gradle-project-1.0-SNAPSHOT-all.jar 10All --add-opens are required when using Java 17 (see JavaModuleOptions).If you want to use Java 8, you should change jvmToolchain version (in build.gradle.kts).For Java 8 you can skip those options:java -jar build/libs/example-spark-kotlin-gradle-project-1.0-SNAPSHOT-all.jar 10IntelliJ IDEAYou can also run it via IntelliJ IDEA. If you generate a project, you should have such a configuration ready to use.The only thing you will have to change is to add all those --add-opens options in VM options.Output of the jobIf everything goes well, running such a job should generate logs like the below:WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties23/02/13 15:32:57 INFO SparkContext: Running Spark version 3.3.123/02/13 15:32:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable23/02/13 15:32:57 INFO ResourceUtils: ==============================================================23/02/13 15:32:57 INFO ResourceUtils: No custom resources configured for spark.driver.23/02/13 15:32:57 INFO ResourceUtils: ==============================================================23/02/13 15:32:57 INFO SparkContext: Submitted application: Example Spark-Kotlin-Gradle project23/02/13 15:32:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -&gt; name: cores, amount: 1, script: , vendor: , memory -&gt; name: memory, amount: 1024, script: , vendor: , offHeap -&gt; name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -&gt; name: cpus, amount: 1.0)23/02/13 15:32:57 INFO ResourceProfile: Limiting resource is cpu23/02/13 15:32:57 INFO ResourceProfileManager: Added ResourceProfile id: 023/02/13 15:32:57 INFO SecurityManager: Changing view acls to: jakub.pradzynski23/02/13 15:32:57 INFO SecurityManager: Changing modify acls to: jakub.pradzynski23/02/13 15:32:57 INFO SecurityManager: Changing view acls groups to:23/02/13 15:32:57 INFO SecurityManager: Changing modify acls groups to:23/02/13 15:32:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(jakub.pradzynski); groups with view permissions: Set(); users with modify permissions: Set(jakub.pradzynski); groups with modify permissions: Set()23/02/13 15:32:57 INFO Utils: Successfully started service 'sparkDriver' on port 61440.23/02/13 15:32:57 INFO SparkEnv: Registering MapOutputTracker23/02/13 15:32:57 INFO SparkEnv: Registering BlockManagerMaster23/02/13 15:32:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information23/02/13 15:32:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up23/02/13 15:32:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat23/02/13 15:32:57 INFO DiskBlockManager: Created local directory at /private/var/folders/yz/624nvk5d3nx111xksfv3b_vc0000gq/T/blockmgr-09e80830-9d54-4307-90a6-12a49829a29823/02/13 15:32:57 INFO MemoryStore: MemoryStore started with capacity 4.6 GiB23/02/13 15:32:57 INFO SparkEnv: Registering OutputCommitCoordinator23/02/13 15:32:57 INFO Utils: Successfully started service 'SparkUI' on port 4040.23/02/13 15:32:57 INFO Executor: Starting executor ID driver on host 192.168.1.3523/02/13 15:32:57 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''23/02/13 15:32:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61441.23/02/13 15:32:57 INFO NettyBlockTransferService: Server created on 192.168.1.35:6144123/02/13 15:32:57 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy23/02/13 15:32:57 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.35, 61441, None)23/02/13 15:32:57 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.35:61441 with 4.6 GiB RAM, BlockManagerId(driver, 192.168.1.35, 61441, None)23/02/13 15:32:57 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.35, 61441, None)23/02/13 15:32:57 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.35, 61441, None)Staring example Kotlin-Spark projectProgram arguments: 10Sum of random prices: 664.77$SummaryThat‚Äôs it. Now you have prepared infrastructure to create more complex Apache Spark jobs (with Kotlin &amp; Gradle).For more information, I refer you to the documentation: Apache Spark Kotlin for Apache Spark Quick Start Guide kotlin-spark-example KotestI also encourage you to visit my blog regularly.I will write more Apache Spark posts in the future, including how to deploy such a project on GCP." }, { "title": "Do you write scripts? Consider using Spring Shell for it!", "url": "/posts/consider-spring-shell-for-work-scripts/", "categories": "IT, Spring Framework", "tags": "it, spring shell", "date": "2023-02-08 07:00:00 +0000", "snippet": "IntroductionAre you one of those who prefer to spend 5 hours writing a script for something that takes 10 minutes?Me too!Sometimes, when I want to write a new script, I decide to do this in a new l...", "content": "IntroductionAre you one of those who prefer to spend 5 hours writing a script for something that takes 10 minutes?Me too!Sometimes, when I want to write a new script, I decide to do this in a new language, e.g., GO.Most of the time, it is enjoyable and educational but also very ineffective.A few weeks ago, I found a post about Spring Shell supportingSpring Boot 3.Previously, I didn‚Äôt use Spring Shell, so it seemed interestingto me.I decided to test it a little and consider using it daily. And I think you should do this too üòâYou can prepare your own shell with defined commands to execute many times thanks to Spring Shell.You can run it directly from the command line in non-interactive or interactive mode.When you collect many scripts that help you work daily, you can have opened shell in your terminal to run some steps quickly.Pros of using Spring ShellThe same technology stack as in everyday workIf you work with: JVM languages (e.g., Java, Kotlin) Spring FrameworkYou can write scripts in the same stack daily.For example, you can use Spring Data to connect to a database.You have access to everything that Spring offers. Also, you can generate new Spring Shell projects just byusing start.spring.io.Testing scripts or even TDDHow often do you write tests for your scripts? I thought so.When you use generated Spring Shell project, you have prepared the infrastructure for tests.You can either code something and write a test or even use TDD for it.Script should be in service - easy to migrateDo you write scripts at work? For example, when you have to prepare some data for testing in many microservices?Or do some technical stuff, but automatically.When you work in Spring Framework and decide that script or part of it should be in the service repository, you can copy and reuse it.Declarative HTTP client from Spring 6In previous post I describe how use Spring Shell withdeclarative HTTP client available since Spring 6.It makes sending HTTP requests from such a shell script very easy.Extended scripts with FlowDo you want to write a script that allows doing something step by step, collecting another user input?Spring Shell allows creation flow easily.Thanks to that, you will have a script that executes the next step after the following user input.SummaryOf course, there are some cons of using Spring Shell.For example, it‚Äôs a whole shell, not a single script. It‚Äôs Spring Framework. It may be extensive.Also, there may be better choices when most of your scripts use bash commands.Maybe instead of that, you should look at zx.I don‚Äôt try to convince you that Spring Shell is the best choice. It may not.I want to show you some pros, so you can decide if it will be a handful in your work.I will try Spring Shell more and see how it will work after a few months." }, { "title": "Using declarative HTTP client in Spring Shell project with Kotlin", "url": "/posts/spring-shell-with-declarative-http-client-in-kotlin/", "categories": "IT, Spring Framework", "tags": "it, spring shell, declarative http client, kotlin", "date": "2023-02-03 06:00:00 +0000", "snippet": "IntroductionSince January 25, 2023, we got new version of Spring Shell -3.0.0.This release uses Spring Boot 3.0.2. Hence, Spring 6. Hence, we can use declarative HTTPclient.Project setupYou can gen...", "content": "IntroductionSince January 25, 2023, we got new version of Spring Shell -3.0.0.This release uses Spring Boot 3.0.2. Hence, Spring 6. Hence, we can use declarative HTTPclient.Project setupYou can generate a new Spring Shell project using start.spring.io.It does not matter how you create a project.It is essential to have declared dependencies: implementation(\"org.springframework.shell:spring-shell-starter\") // version above 3.0.0 implementation(\"org.springframework.boot:spring-boot-starter-webflux\") // version above 3.0.2Creating a sample declarative HTTP clientFor example, we create an HTTP client for GitHub to fetch a list of repositories.@HttpExchange(url = \"https://api.github.com\")internal interface GithubClient { @GetExchange(\"/users/{username}/repos\") fun getUserRepos( @PathVariable username: String, @RequestParam page: Int = 1, @RequestParam per_page: Int = 100, @RequestHeader(\"Authorization\") token: String, @RequestHeader(\"Accept\") accept: String = \"application/vnd.github+json\", @RequestHeader(\"X-GitHub-Api-Version\") githubApiVersion: String = \"2022-11-28\", ): List&lt;Repo&gt;}data class Repo(val id: String, val name: String)As you can see, Spring 6 allows us to create an interface with a declaration of requests we want to send.We can define essential things for an HTTP request, such as URL, path, path variables, query params, and headers.If you are interested in this topic, I encourage you to watch the video: üöÄ New in Spring Framework 6: HTTP Interfaces.Now, we have to create a bean from that interface.@Configurationinternal class GithubClientConfig { @Bean(name = [\"githubWebClient\"]) fun webClient(): WebClient = WebClient.builder() .exchangeStrategies(exchangeStrategies()) .build() @Bean fun githubClient(@Qualifier(\"githubWebClient\") webClient: WebClient): GithubClient = HttpServiceProxyFactory .builder(WebClientAdapter.forClient(webClient)) .build() .createClient(GithubClient::class.java) private fun exchangeStrategies(): ExchangeStrategies { val objectMapper = ObjectMapper() .configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false) .registerModule(KotlinModule.Builder().build()) return ExchangeStrategies .builder() .codecs { configurer: ClientCodecConfigurer -&gt; configurer.defaultCodecs().jackson2JsonEncoder(Jackson2JsonEncoder(objectMapper)) configurer.defaultCodecs().jackson2JsonDecoder(Jackson2JsonDecoder(objectMapper)) }.build() }}For declarative HTTP clients, it is essential to use HttpServiceProxyFactory with WebClient to create a new instance of GithubClient.ExchangeStrategies are to support Kotlin to JSON conversion.We must remember that WebFlux project is quite big and has some requirements.For example, we will see log `Netty started on port 8080 after starting the shell.I may limit the required dependency for declarative HTTP clients in the future.Create a sample Shell CommandNow, we can create a command in our project:@ShellComponentinternal class GithubSampleCommand(private val githubClient: GithubClient) { @ShellMethod fun userRepos(username: String): List&lt;Repo&gt; = githubClient.getUserRepos( username = username, token = \"Bearer {your_github_token}\" )}And that‚Äôs all.We can run our project. Using help command in shell we can see the following:...Github Sample Command user-repos:And after running the command user-repos --username jakubpradzynski, we will see the output:[Repo(id=226322355, name=advent-of-code-2019), Repo(id=139764856, name=azariah), Repo(id=124221129, name=crispus), Repo(id=192063330, name=digits-predictor), Repo(id=377232244, name=fake-news-predictor), Repo(id=238491138, name=google_tasks_cli), Repo(id=242139319, name=google_translate_cli), Repo(id=515534896, name=jakubpradzynski.github.io), Repo(id=545125921, name=jvm-bloggers), Repo(id=549828075, name=mongodb-spring-kurs-dla-poczatkujacych), Repo(id=461206656, name=valuable-study-materials-it)]If you want to know more about Spring Shell, I recommend reading documentation.What‚Äôs next?I‚Äôve been testing Spring Shell recently.When I saw that it supports Spring 6 it seemed interesting to me.I will probably use more of it and describe it in future posts.Stay tuned." }, { "title": "Setting readPreference option in Spring Data MongoDB projects", "url": "/posts/mongodb-secondary-reads-in-spring-project/", "categories": "IT, MongoDB", "tags": "it, mongodb, read preference, spring data", "date": "2022-10-10 02:00:00 +0000", "snippet": "IntroductionIn previous post I wrote about MongoDB optimization by reading from secondary nodes.In this post I will show you how to use this option in Spring Boot project.Using secondary reads glob...", "content": "IntroductionIn previous post I wrote about MongoDB optimization by reading from secondary nodes.In this post I will show you how to use this option in Spring Boot project.Using secondary reads globallyIf you use Spring Data MongoDB on daily basis, you probably create MongoRepository like this:@Document(collection = \"books\")data class Book(@Id val id: String, val title: String)interface BooksRepository : MongoRepository&lt;Book, String&gt;SimpleMongoRepository which is basic implementation of MongoRepository uses MongoTemplate to execute queries.So if you want to switch all reads to for example secondaryPreferred you can just create new bean with the readPreference option set properly.Like this:@Configurationclass MongoConfig { @Bean fun mongoTemplate(@Value(\"\\${mongoUri}\") mongoUri: String): MongoTemplate { val mongoTemplate = MongoTemplate(SimpleMongoClientDatabaseFactory(mongoUri)) mongoTemplate.setReadPreference(ReadPreference.secondaryPreferred()) return mongoTemplate }}After setting this bean in your project all repositories will use MongoTemplate which will execute queries with readPreference setto secondaryPreferred.Using secondary reads on specific queriesUnfortunately, there is no option to set readPreference option on specific queries.Or maybe only I don‚Äôt know how to do that and you can give me a hint in comments.The only option which I found is to use annotationinterface BooksRepository : MongoRepository&lt;Book, String&gt; { @Meta(flags = [CursorOption.SECONDARY_READS]) override fun findAll(): List&lt;Book&gt;}Or if you code queries by your own using MongoOperations or FluentMongoOperations you can allow secondary reads like that:@Repositoryclass MyBooksRepository(private val mongoOperations: MongoOperations) { fun findAll(): List&lt;Book&gt; = mongoOperations.find( Query .query(Criteria()) .allowSecondaryReads(), Book::class.java )}But be careful!This option set readPreference only to primaryPreferred (you can see itin MongoTemplate implementation).This means that queries will be executed on primary anyway, unless primary becomes unavailable.So if you want to execute some of queries with different readPrefence you have to create many MongoTemplate with option set appropriately.Then use the appropriate MongoTemplate for specific queries." }, { "title": "How to easily optimize replica set resource consumption & performance in MongoDB?", "url": "/posts/mongodb-secondary-reads/", "categories": "IT, MongoDB", "tags": "it, mongodb, replica set, optimization, read preference", "date": "2022-10-08 02:00:00 +0000", "snippet": "SetupI created new free MongoDB cluster on Atlasand loaded sample data into this cluster.As you can see cluster: has version 5.0.13 is deployed on GCP / Belgium contains 3 nodes in Replica SetSo...", "content": "SetupI created new free MongoDB cluster on Atlasand loaded sample data into this cluster.As you can see cluster: has version 5.0.13 is deployed on GCP / Belgium contains 3 nodes in Replica SetSo, what is this ‚Äúreplica set‚Äù?Replica setAccording to documentation: A replica set in MongoDB is a group of mongod processesthat maintain the same data set.Thanks to that we can get redundancy and high availability.Always one of the nodes in replica set is selected as primary and others are called secondary or sometimes arbiter.But we will not focus on what type is responsible for.The most important things to remember are: All writes goes through primary and are replicated to secondaries Reads can be executed both on primary and on secondaries When current primary will stop responding, new one will be selectedIf you want to learn more, I recommend that you check the documentation.Read preferenceAs I mention above, reads can be executed from primary or secondary.Read preference is option which lets us specify where reads operations should be sentModesAvailable modes for readPreference are: primary - reads only from primary node primaryPreferred - mostly from primary, if unavailable then from secondaries secondary - reads only from secondary nodes secondaryPreferred - mostly from secondaries, if unavailable then from primary nearest - reads from selected replica set member based onlatency (details)Default modeWhen you connect to yours MongoDB cluster by default primary read preference is used.I‚Äôve created simple script to generate traffic on my cluster:db = connect('${mongodb_uri}');function delay(time) { return new Promise(resolve =&gt; setTimeout(resolve, time));}for (let i = 0; i &lt; 1_000; i++) { delay(500).then(() =&gt; db.posts.findOne({}));}After running it, metrics look like:So, all reads were made from primary node (it is marked by letter P above chart).Reads from secondaryWhen I simply modify script (only but adding ?readPreference=secondary at the end of the mongo_uri) and run it again,I can see difference on the charts:Read operations were made from secondary nodes, so now the whole read traffic to database is split to two secondaries.Now primary will be responsible only for write operations. It should have significant impact on resource consumption and database performance.Warnings This may not be the best choice for everyone. Replication from primary to secondaries may take some time or even fails. It will depend on thereplica set configuration. Few use cases for various modes are describedin MongoDB documentation. If you have MongoDB in version below 4.0, reading from secondaries may lead to slower responses from database. It‚Äôs because that in previousversions reading from secondaries was blocking. In 4.0 version Non-blocking secondary reads have been introduced (moredetails here). " }, { "title": "Be careful with saveAll method in spring-boot-data-mongodb!", "url": "/posts/save-all-in-spring-boot-data-mongodb/", "categories": "IT, MongoDB", "tags": "it, mongodb, spring data", "date": "2022-10-03 14:00:00 +0000", "snippet": "IntroductionWhen we work in project based on Spring Framework and need to accessdatabase we will probably use Spring Data for this.One of the benefits of this approach is that we can only configure...", "content": "IntroductionWhen we work in project based on Spring Framework and need to accessdatabase we will probably use Spring Data for this.One of the benefits of this approach is that we can only configure connection to database (e.g. in properties) and thenjust create Bean of interface implementing Repository, CrudRepository or PagingAndSortingRepository.This interfaces provides us magically methods to operate on our database.For example CrudRepository will provide us implementation to all methodsdeclared hereoperating on database which we chose.Are such auto-generated methods are flawless? No! And I will show you why on specific example.Example MongoDB repositoryWhat is MongoRepository?In spring-boot-data-mongodb project we can find additional Repository interface namedMongoRepository.It is special interface for MongoDB with base implementationSimpleMongoRepository.Example collectionLet‚Äôs assume we have project which stores books. Such project will store objects in MongoDB collection books.Class definition for books will look like:@Document(collection = \"books\")data class Book(@Id val id: String? = null, val name: String)If you wonder why id is nullable just wait a little üòâFor this collection we will define repository:interface BooksRepository : MongoRepository&lt;Book, String&gt;And that‚Äôs it. Now we can use our new repository in project as Spring Bean.We have methods like save, find, delete, count out of the box. Isn‚Äôt that cool?Problem with saveAll method in SimpleMongoRepositoryid is essential!Let‚Äôs look at such two tests:@Testfun shouldSaveBooks_WithoutId() { // GIVEN val books = listOf( Book(name = \"bookName1\"), Book(name = \"bookName2\") ) // WHEN booksRepository.saveAll(books) // THEN assertEquals(booksRepository.findAll(), books)}@Testfun shouldSaveBooks_WithId() { // GIVEN val books = listOf( Book(id = \"bookId1\", name = \"bookName1\"), Book(id = \"bookId2\", name = \"bookName2\") ) // WHEN booksRepository.saveAll(books) // THEN assertEquals(booksRepository.findAll(), books)}Similar, right?The only difference is that in the second one we fill the id field before saving.Let‚Äôs look at the results of this tests.Test in which we fill the ids passed. That because we compared whole objects.When we leave ids as null MongoDB will set them itself. You can see it when we look at the assertion which failed.It‚Äôs clear. Each document needs an ID so MongoDB set it when necessary (more details about _idfield here).But there is one thing that surprises when we look at the application logs while executing the tests.To see this we have to enable DEBUG logging on MongoTemplate(add logging.level.org.springframework.data.mongodb.core.MongoTemplate=DEBUG in application.properties).Test in which we set ids ourselves generates two queries into database (saves):And test without ids generates only one query into database (insert):Why is that?saveAll exposed. /* * (non-Javadoc) * @see org.springframework.data.mongodb.repository.MongoRepository#saveAll(java.lang.Iterable) */@Overridepublic&lt;S extends T&gt; List&lt;S&gt; saveAll(Iterable&lt;S&gt; entities){ Assert.notNull(entities,\"The given Iterable of entities not be null!\"); Streamable&lt;S&gt; source=Streamable.of(entities); boolean allNew=source.stream().allMatch(entityInformation::isNew); if(allNew){ List&lt;S&gt; result=source.stream().collect(Collectors.toList()); return new ArrayList&lt;&gt;(mongoOperations.insert(result,entityInformation.getCollectionName())); } return source.stream().map(this::save).collect(Collectors.toList());}This is body of the saveAll method from SimpleMongoRepository.When all entities are new (checking the id), all of them are collecting to list and save using one insert method.Otherwise, each entity is save separately.Why is that?Because when we not fill the _id field it means that all documents should be inserted without collision to any existing one.But when we fill the _id there may be a situation that document with such ID already exists.Then what?We will insert new document or update existing one.MongoDB does not provide method for saving all given documents.We can use only insert,insertManyor save.That‚Äôs why saveAll method in MongoRepository has such logic.It covers something that we may want to use in our service but it is not available in MongoDB itself.How does it affect performance?Let‚Äôs create two more tests to see how it affects performance.Similar as above, we will create tests with and without ids, but this time we will save 10,000 books.So, the tests will look like: @Testfun shouldSaveTenThousandOfBooks_WithoutId() { // GIVEN val books = (1..10_000) .map { Book(name = \"bookName$it\") } // WHEN booksRepository.saveAll(books) // THEN assertEquals(booksRepository.count(), 10_000)} @Testfun shouldSaveTenThousandOfBooks_WithId() { // GIVEN val books = (1..10_000) .map { Book(id = \"bookId$it\", name = \"bookName$it\") } // WHEN booksRepository.saveAll(books) // THEN assertEquals(booksRepository.count(), 10_000)}What are the test results?Of course both tests passed, because we only checked count of saved objects.But look at the execution times‚Ä¶üö® Saving books without id took 560ms and with id 17 sec 197 ms. üö®üö® That‚Äôs about 30 times more! üö®This is a really significant difference, especially in online systems.Example case when this can cause a problemSuppose you are creating a service that imports flat files (csv, xlsx) into the database.The application receives the file, has to read all the lines, map the data and save it in the database.The time to load a single file (and the database load) will be significantly greater if you give each document _id immediately in the service whenmapping data.For files with thousands of lines, you will run thousands of database queries instead of one.Complaints from admins about the slow-running system guaranteed üòâConclusion When it is possible, avoid setting _id manually in code using saveAll method from MongoRepository. Abstractions which ‚Äúdo the magic stuff‚Äù to make our coding simpler is usually awsome but sometimes it can be a burden. " }, { "title": "Creating MongoDB case insensitive index in Spring based project", "url": "/posts/mongodb-spring-creating-case-insensitive-index/", "categories": "IT, MongoDB", "tags": "it, mongodb, spring, index, compound index, case sensitiveness", "date": "2022-07-28 15:45:00 +0000", "snippet": "Example problem to solveOne of the most popular class in our services is User.The user may enter his nickname during registration.Let‚Äôs assume that our business requirement is: user should be uniq...", "content": "Example problem to solveOne of the most popular class in our services is User.The user may enter his nickname during registration.Let‚Äôs assume that our business requirement is: user should be unique by his nicknameSolution in MongoDB - case insensitive indexIf we use MongoDB, we can use unique index with appropriate option to achieve that requirement.The appropriate optionis collation withwith set fields locale and strength.Supported values for this fields you can find here: locale strengthFor this article let‚Äôs assume that we want to focus on language English (United States) so we will use en_US valuefor locale.For strength we should use values 1 or 2 because this levels means that values comparison will ignore case.So, our solution directly on MongoDB will look like:db.users.createIndex( { nickname: 1}, { unique: true, collation: { locale: 'en_US', strength: 1 } } )How to create case insensitive index in MongoDB using Spring Framework?Usually for creating indexes we can use annotations: @Indexedon field @CompoundIndexon classThey have option for uniqueness to set but not for collation.Fortunately, such option is available if we create index directly via MongoTemplate.So, with such simple code we can meet the requirement.@Configuration@DependsOn(\"mongoTemplate\")class CollectionConfig(@Autowired private val mongoTemplate: MongoTemplate) { @PostConstruct fun ensureIndexes() { mongoTemplate.indexOps(User::class.java).ensureIndex( Index(\"username\", Sort.Direction.ASC) .unique() .collation( of(Locale.US).strength(ComparisonLevel.primary()) ) ) }}Issue in spring-boot-mongodb projectI‚Äôve created issue to add parameters in @Indexed and @CompoundIndex annotations for setting collation value.You can watch this issue if you are interested in what will happen next.Sources Case Insensitive Index in MongoDB documentation. Question onStackOverflow with mine answer as well." }, { "title": "Welcome!", "url": "/posts/first-post/", "categories": "Blogging", "tags": "first post", "date": "2022-07-19 05:00:00 +0000", "snippet": "This is my first blog post. It won‚Äôt be long. I just want to say hello!About meInformation about me is available on the subpage ABOUT so I won‚Äôt duplicate it.Mainly because that page will be update...", "content": "This is my first blog post. It won‚Äôt be long. I just want to say hello!About meInformation about me is available on the subpage ABOUT so I won‚Äôt duplicate it.Mainly because that page will be updated and the post could expire.What I want to do on this blog?I work in IT as a developer so probably the most part of the blog will be around IT.But maybe I will make some posts about my hobby: finance &amp; investing." } ]
